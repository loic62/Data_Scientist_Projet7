{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies génériques\n",
    "import numpy as np\n",
    "import sys\n",
    "import io\n",
    "import os\n",
    "import time\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'sagemaker'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-3bc94b926f04>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mimport\u001b[0m \u001b[0msagemaker\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mboto3\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSparkContext\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mSparkConf\u001b[0m   \u001b[1;31m# 'SparkContext' permet d'instancier des RDD\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'sagemaker'"
     ]
    }
   ],
   "source": [
    "import sagemaker\n",
    "import boto3\n",
    "import pyspark\n",
    "\n",
    "from pyspark import SparkContext              # 'SparkContext' permet d'instancier des RDD\n",
    "from pyspark.sql import SparkSession          # 'SparkSession' permet d'instancier des objets de type DataFrame\n",
    "from pyspark.sql import Row\n",
    "from pyspark.sql.functions import udf\n",
    "\n",
    "from pyspark.sql.types import ArrayType, StringType, IntegerType\n",
    "\n",
    "# region = \"eu-west-1\"   # Ireland: la moins couteuse. Indique sur quel data center je suis connecté."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Librairies pour le traitement d'images\n",
    "from PIL import Image\n",
    "import cv2\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mybucket = \"lcanonne-p8\"\n",
    "prefix   = \"images\"\n",
    "folder   = \"s3://{}/{}/\".format(mybucket, prefix)\n",
    "\n",
    "global image1   # globale variable to use in read_image and gest_desc functions\n",
    "\n",
    "# avocat_data = 's3://{}/{}/{}'.format(mybucket, prefix, 'Avocado')\n",
    "# banane_data = 's3://{}/{}/{}'.format(mybucket, prefix, 'Banana')\n",
    "# print(avocat_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init S3_client and S3 sagemaker region  and boto3 Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region = boto3.Session().region_name\n",
    "session = boto3.session.Session(region_name=region)    # Ouverture d'une Session sur la région 'eu-west-1' (Ireland)\n",
    "s3_client = session.client('s3')                       # indique que le client utilise le service S3 de AWS\n",
    "s3 = boto3.resource(\"s3\", region_name=region)          # référence sur le service S3\n",
    "\n",
    "# Création d'une session customisée pour indiquer la region AWS. On ne prend pas la session par défaut."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fonctions (load, read, extract, desc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création du dataframe avec la 1ere colonne qui est le nom complet de l'image dans le bucket.\n",
    "\n",
    "def load_datas(folder, s3_client, sc):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    sub_folders = s3_client.list_objects_v2(Bucket=mybucket, Prefix=prefix)\n",
    "    # Limite la réponse aux clés qui commencent par la valeur de 'prefix' sur le point d'accés de 'mybucket'\n",
    "    if \"Contents\" not in sub_folders:\n",
    "        print(\"dossier source non trouvé\")\n",
    "        sys.exit(0)\n",
    "\n",
    "    lst_path = []\n",
    "\n",
    "    for key in sub_folders[\"Contents\"]:\n",
    "        file = key[\"Key\"]\n",
    "        if ('jpg' in file):             # j'ai remplacé le filter avec cet if (pr filtrer que les images)\n",
    "            file = file.replace(prefix + \"/\", \"\")\n",
    "            lst_path.append(folder+file)\n",
    "            \n",
    "    print(\"Nombre d'images chargées :\", len(lst_path))\n",
    "    \n",
    "    rdd = sc.parallelize(lst_path)                         # SparkContext utilisé pour créer un objet RDD\n",
    "    row_rdd = rdd.map(lambda x: Row(x))                    # rdd doit être constitué d'objet de type 'Row'\n",
    "    df = spark.createDataFrame(row_rdd, [\"path_img\"])      # Enregistrement des images dans un Dataframe PySpark\n",
    "                                                           # <=> création d'un dataframe à partir d'un RDD\n",
    "\n",
    "    print(\"Temps execution %sec ---\" % (time.time() - start_time))\n",
    "    return df\n",
    "\n",
    "\n",
    "# Paramètre possible de parallelize() :  dist, list, tuple, set.  Ici c'est la 'list' qui est utilisée.\n",
    "# La méthode map() est une transformation RDD qui s'applique à chaque élément du RDD via une fonction lambda\n",
    "\n",
    "# parallelize() transforme cette list en un ensemble distribué de paths et nous offre toutes les possibilités \n",
    "#    de l’infrastructure de Spark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2eme colonne du dataframe:  catégorie\n",
    "\n",
    "def extract_categ(path):\n",
    "    lst_file = path.split('/')\n",
    "    categ  = lst_file[-2]\n",
    "    return categ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3eme colonne du dataframe:  matrice de chaque image mise à plat\n",
    "\n",
    "def read_image(img):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    global image1   # init a global variable (image1) to pass in the next function (get_dec)\n",
    "    \n",
    "    img = img.replace(\"s3://{}/\".format(mybucket), \"\")\n",
    "    s3 = boto3.resource(\"s3\", region_name=region)\n",
    "    bucket = s3.Bucket(mybucket)\n",
    "    \n",
    "    object = bucket.Object(img)\n",
    "    response = object.get()\n",
    "    file_stream = response['Body']\n",
    "    image = Image.open(file_stream)\n",
    "    \n",
    "    if image is None:\n",
    "        image = 0\n",
    "    else:\n",
    "        image = np.asarray(image)\n",
    "        image1 = image\n",
    "        image = image.flatten().tolist()\n",
    "\n",
    "    print(\"Temps execution %sec ---\" % (time.time() - start_time))\n",
    "                           \n",
    "    return image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_desc(img):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    read_image(img)\n",
    "    image = image1\n",
    "    \n",
    "    orb = cv2.ORB_create(nfeatures=50)\n",
    "    keypoints, desc = orb.detectAndCompute(image, None)\n",
    "    print(\"Temps execution %sec ---\" % (time.time() - start_time))\n",
    "    \n",
    "    if desc is None:\n",
    "        desc = 0\n",
    "    else:\n",
    "        desc = desc.flatten().tolist()\n",
    "        \n",
    "    return desc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Init Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sc = SparkContext.getOrCreate()   # Instantiation d'un SparkContext\n",
    "sc.setLogLevel(\"Warn\")\n",
    "\n",
    "spark = SparkSession.builder.appName('projet8').getOrCreate()  # retourne un objet de type 'SparkSession'\n",
    "\n",
    "# sc : client qui se connecte à un cluster Spark\n",
    "# Un objet SparkSession permet d'instancier des objets de type DataFrame\n",
    "# Un objet SparkContext permet d'instancier des RDD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load, Transform and Extract data with Spark (udf: rdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_datas(folder, s3_client, sc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remplissage de la 2eme colonne 'categ'\n",
    "\n",
    "udf_categ = udf(extract_categ, StringType())         # Crée un UDF avec la fonction 'extract_categ' avec StringType()\n",
    "                                                     #   comme type de retour. C'est le type par défaut.\n",
    "df = df.withColumn(\"categ\", udf_categ(\"path_img\"))   # Ajout de la colonne 'categ' au dataframe avec la valeur de udf_categ()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remplissage de la 3eme colonne 'image'. Chaque image a été transformé en array numpy puis vecteur unitaire.\n",
    "\n",
    "udf_image = udf(read_image, ArrayType(IntegerType()))\n",
    "df = df.withColumn(\"image\", udf_image(\"path_img\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.image.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remplissage de la 4eme colonne 'descriptors' avec les descripteurs de chaque image transformés en vecteur unitaire\n",
    "udf_desc = udf(get_desc, ArrayType(IntegerType()))\n",
    "df = df.withColumn(\"descriptors\", udf_desc(\"path_img\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.filter(df.descriptors.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.show(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save df in format parquet  then copy to s3 with sagemaker session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.repartition(1).write.mode('overwrite').parquet('resultat')   # resultat = name  of folder where the dataframe\n",
    "                                                                # will be stored in sagemaker instance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upload_data = sagemaker.Session().upload_data(bucket=mybucket, \n",
    "                                              path='resultat',             # local file in sagemaker instance\n",
    "                                              key_prefix='outputresultat') # bucket where we stored parquet in s3\n",
    "print('upload_data: {}'.format(upload_data))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
